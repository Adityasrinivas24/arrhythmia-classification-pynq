{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a0186-8763-4aeb-917b-21d12deb76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_arch_list())\n",
    "\n",
    "print(f\"Is CUDA supported by this system?{torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    " \n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "       \n",
    "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c84f9-aad1-4b09-893b-6b3f6acbb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58691af-811a-4965-b9e1-4050664ae31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from brevitas.nn import QuantConv2d, QuantReLU, QuantLinear\n",
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
    "from brevitas.quant.scaled_int import Int8WeightPerTensorFloat\n",
    "from brevitas.quant.scaled_int import Int8BiasPerTensorFloatInternalScaling\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        myWeight_bit_width = 4\n",
    "        act_bit_width = 4\n",
    "\n",
    "        self.inverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "        self.conv1 = QuantConv2d(3, 32, kernel_size=3, stride=2, padding=1,  bias=True, input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,\n",
    "                        weight_quant=Int8WeightPerTensorFloat, bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=True,weight_bit_width=myWeight_bit_width)\n",
    "        self.relu = QuantReLU(bit_width=act_bit_width, max_val=6, act_quant=Int8ActPerTensorFloat, \n",
    "                      input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,return_quant_tensor=True)\n",
    "        self.features = self._make_layers(in_channels=32)\n",
    "        self.conv_last = QuantConv2d(320, 1280, kernel_size=1,  bias=True, input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,\n",
    "                        weight_quant=Int8WeightPerTensorFloat, bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=True,weight_bit_width=myWeight_bit_width)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = QuantLinear(1280, 2,  bias=True, input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,\n",
    "                        weight_quant=Int8WeightPerTensorFloat, bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=True,weight_bit_width=myWeight_bit_width)\n",
    "\n",
    "    def _make_layers(self, in_channels):\n",
    "        layers = []\n",
    "        for t, c, n, s in self.inverted_residual_setting:\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    layers.append(QuantBottleneck(in_channels, c, s, t))\n",
    "                else:\n",
    "                    layers.append(QuantBottleneck(in_channels, c, 1, t))\n",
    "                in_channels = c\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.features(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QuantBottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, expansion):\n",
    "        super(QuantBottleneck, self).__init__()\n",
    "        \n",
    "        myWeight_bit_width = 4\n",
    "        act_bit_width = 4\n",
    "\n",
    "        self.stride = stride\n",
    "        planes = expansion * in_planes\n",
    "        self.conv1 = QuantConv2d(in_planes, planes, kernel_size=1, bias=True, input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,\n",
    "                        weight_quant=Int8WeightPerTensorFloat, bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=True,weight_bit_width=myWeight_bit_width)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = QuantReLU(bit_width=act_bit_width, max_val=6, act_quant=Int8ActPerTensorFloat, \n",
    "                      input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,return_quant_tensor=True)\n",
    "        self.conv2 = QuantConv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=True,\n",
    "                                  weight_quant=Int8WeightPerTensorFloat, \n",
    "                                  act_quant=Int8ActPerTensorFloat)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = QuantReLU(bit_width=act_bit_width, max_val=6, act_quant=Int8ActPerTensorFloat, \n",
    "                      input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,return_quant_tensor=True)\n",
    "        self.conv3 = QuantConv2d(planes, out_planes, kernel_size=1, bias=True, input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,\n",
    "                        weight_quant=Int8WeightPerTensorFloat, bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=True,weight_bit_width=myWeight_bit_width)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                QuantConv2d(in_planes, out_planes, kernel_size=1, stride=stride,  bias=True, input_quant=Int8ActPerTensorFloat,output_quant=Int8ActPerTensorFloat,\n",
    "                        weight_quant=Int8WeightPerTensorFloat, bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=True,weight_bit_width=myWeight_bit_width),\n",
    "                nn.BatchNorm2d(out_planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x) if self.stride == 1 else out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8be48e-6f33-45a7-80c1-1ff94c72e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"trained_models/mobilenetv2_w4a4_model.pth\"\n",
    "model = MobileNetV2()\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7bc6b-b801-4f40-9203-618c5cdb0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "     print(f\"Layer: {name}, Type: {param.dtype}, Size: {param.nelement()*param.element_size()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeccb70-ecca-4c51-a133-b7cf9d24073e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883bca7-ae80-498e-8d27-2a07e52ed582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from finn.util.basic import make_build_dir\n",
    "import os\n",
    "    \n",
    "build_dir = os.environ[\"FINN_ROOT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff266d6-b9d6-4397-ba7a-daef5ec3265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import torch\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "model_for_export = model.cpu()\n",
    "\n",
    "export_onnx_path = 'onnx_models/modbilenetv2_w4a4.onnx'\n",
    "input_t = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "export_onnx(model_for_export,export_path = export_onnx_path,input_t = input_t)\n",
    "qonnx_cleanup(export_onnx_path,out_file = export_onnx_path)\n",
    "\n",
    "showInNetron(export_onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e2fda-02c1-4062-ac0e-3c6338de1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap the onnx model to work with Finn\n",
    "\n",
    "onnx_model = ModelWrapper(export_onnx_path)\n",
    "\n",
    "export_finn_onnx_path = 'onnx_models/mobilenetv2_w4a4_finn.onnx'\n",
    "\n",
    "\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(export_finn_onnx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ec90b-2abe-4c66-ac72-9312c37f60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(export_finn_onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0f176-7433-4f16-b034-6279c4eb6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations and Tidy up\n",
    "\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model.save(\"onnx_models/mobilenetv2_w4a4_tidy.onnx\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe12b9-bee7-44d1-821e-907f6ccdc296",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66ad02-88a2-4e48-8200-8bbf0efff66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(\"onnx_models/mobilenetv2_w4a4_tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0],name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"onnx_models/mobilenetv2_w4a4_preproc.onnx\"\n",
    "\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "\n",
    "# join preprocessing and core model\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "model.save(\"onnx_models/mobilenetv2_w4a4_with_preproc.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27045ba7-e2e2-474d-acd9-59b2bcf6375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_with_preproc.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d19bdd-4d5a-4d15-bbe1-43c9d87fc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"onnx_models/mobilenetv2_w4a4_pre_post.onnx\"\n",
    "\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68bd35-381b-48f9-8978-836676216714",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_pre_post.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471c483-50b4-46ab-81d0-6e81b8c0ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streamline the model\n",
    "\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "#showSrc(Streamline)\n",
    "\n",
    "model = ModelWrapper(\"onnx_models/mobilenetv2_w4a4_pre_post.onnx\")\n",
    "model.transform(MoveScalarLinearPastInvariants())\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "model.save(\"onnx_models/mobilenetv2_w4a4_streamlined.onnx\")\n",
    "\n",
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc585e-d74f-49da-a9a8-c7b57de0f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "\n",
    "# bit of tidy-up\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(\"onnx_models/mobilenetv2_w4a4_ready_for_hw_conversion.onnx\")\n",
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_ready_for_hw_conversion.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde50da6-f951-4119-9946-417264288bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converion to HW layers\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "\n",
    "model = ModelWrapper(\"onnx_models/mobilenetv2_w4a4_ready_for_hw_conversion.onnx\")\n",
    "model = model.transform(to_hw.InferBinaryMatrixVectorActivation())\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model.save(\"onnx_models/mobilenetv2_w4a4_hw_layers.onnx\")\n",
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_hw_layers.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44529355-f85b-47fc-803c-aab8ce43b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "\n",
    "model = ModelWrapper(\"onnx_model/mobilenetv2_w4a4_hw_layers.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "\n",
    "parent_model.save(\"onnx_models/mobilenetv2_w4a4_dataflow_parent.onnx\")\n",
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c3759f-b319-4e80-b2ba-13de7242f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.custom_op.registry import getCustomOp\n",
    "\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "showInNetron(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23892841-5114-47cf-be96-bada3853e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74bb63-1164-4588-af2d-d9ba37f018c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_node = model.get_nodes_by_op_type(\"Thresholding\")[0]\n",
    "thresh_node_inst = getCustomOp(thresh_node)\n",
    "thresh_node_inst.set_nodeattr(\"preferred_impl_style\", \"hls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95beb1-f3c0-4d66-8bdf-fc5d2663b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "model = model.transform(SpecializeLayers())\n",
    "\n",
    "model.save(\"onnx_models/mobilenetv2_w4a4_specialize_layers.onnx\")\n",
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_specialize_layers.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c844b-96d2-44be-9098-fcdc392abc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc0 = model.graph.node[1]\n",
    "fc0w = getCustomOp(fc0)\n",
    "\n",
    "print(\"CustomOp wrapper is of class \" + fc0w.__class__.__name__)\n",
    "\n",
    "fc0w.get_nodeattr_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f738fb-81af-45ba-a336-da55475c5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "# (PE, SIMD, in_fifo_depth, out_fifo_depth, ramstyle) for each layer\n",
    "config = [\n",
    "    (16, 49, [16], [64], \"block\"),\n",
    "    (8, 8, [64], [64], \"auto\"),\n",
    "    (8, 8, [64], [64], \"auto\"),\n",
    "    (10, 8, [64], [10], \"distributed\"),\n",
    "]\n",
    "for fcl, (pe, simd, ififo, ofifo, ramstyle) in zip(fc_layers, config):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepths\", ififo)\n",
    "    fcl_inst.set_nodeattr(\"outFIFODepths\", ofifo)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "    \n",
    "# set parallelism for input quantizer to be same as first layer's SIMD\n",
    "inp_qnt_node = model.get_nodes_by_op_type(\"Thresholding_hls\")[0]\n",
    "inp_qnt = getCustomOp(inp_qnt_node)\n",
    "inp_qnt.set_nodeattr(\"PE\", 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22d597-74ca-4468-a76c-a473ae31424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"onnx_models/mobilenetv2_w4a4_set_folding_factors.onnx\")\n",
    "showInNetron(\"onnx_models/mobilenetv2_w4a4_set_folding_factors.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c468f-29a3-4afe-83e1-9aee979df3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55b8c0-0292-4686-8674-f5349f8fafca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cff665-8aa0-435c-b736-94df2a98c7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e512d0-8ecd-497c-b8bf-aca671c0cbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06282a35-bccf-4681-87bf-4f4e5896541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Inference Cost\n",
    "#First, we have to export the model to Brevitas quantized variant of the ONNX interchange format.\n",
    "#Radio-ml-challenge-21\n",
    "\n",
    "from brevitas.export.onnx.generic.manager import BrevitasONNXManager\n",
    "from finn.util.inference_cost import inference_cost\n",
    "import json\n",
    "\n",
    "export_inference_onnx_path = \"util/inference_cost/model_export.onnx\"\n",
    "final_onnx_path = \"util/inference_cost/model_final.onnx\"\n",
    "cost_dict_path = \"util/inference_cost/model_cost.json\"\n",
    "\n",
    "inference_cost(export_inference_onnx_path, output_json=cost_dict_path, output_onnx=final_onnx_path,preprocess=True, discount_sparsity=True)\n",
    "\n",
    "with open(cost_dict_path, 'r') as f:\n",
    "    inference_cost_dict = json.load(f)\n",
    "\n",
    "bops = int(inference_cost_dict[\"total_bops\"])\n",
    "w_bits = int(inference_cost_dict[\"total_mem_w_bits\"])\n",
    "\n",
    "bops_baseline = 807699904\n",
    "w_bits_baseline = 1244936\n",
    "\n",
    "score = 0.5*(bops/bops_baseline) + 0.5*(w_bits/w_bits_baseline)\n",
    "print(\"Normalized inference cost score: %f\" % score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
